{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":91721,"databundleVersionId":13760552,"sourceType":"competition"}],"dockerImageVersionId":31153,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install --upgrade xgboost scikit-learn\n!pip install optuna","metadata":{"trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"import numpy as np\nimport pandas as pd\n\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\nfrom sklearn.pipeline import Pipeline\nfrom sklearn.compose import ColumnTransformer\nfrom sklearn.preprocessing import OneHotEncoder\nfrom sklearn.model_selection import KFold\n\nfrom sklearn.model_selection import train_test_split\nfrom sklearn.metrics import mean_squared_error\n\nimport xgboost as xgb\nimport optuna\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"_kg_hide-output":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Introduction\n\nThis project tackles the kaggle playground challenge of predicting road accident risk. The goal is to build a model that predicts accident_risk a continuous value between 0 and 1 based on features like road conditions, weather, and time. A successful model can help identify high risk scenarios, allowing for targeted safety interventions to make roads safer.\n\nSince the target is a continuous value, this is a regression problem. We will use the XGBoost Regressor, a powerful gradient-boosting algorithm well-suited for this kind of tabular data. This notebook will document the full process, starting with exploratory data analysis (EDA) to understand the data and guide our preprocessing steps.","metadata":{}},{"cell_type":"code","source":"data_train_path = '/kaggle/input/playground-series-s5e10/train.csv'\ndata_test_path = '/kaggle/input/playground-series-s5e10/test.csv'\n\ndata_train = pd.read_csv(data_train_path)\ndata_test = pd.read_csv(data_test_path)\n\ndata_train.info()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# EDA #\nNext we'll continue in this vain of looking at the data to better understand the data moving forward. This is a crucial step to understand the data's structure, relationships ( between the features and targets ), and find any potential issues. We'll start looking at the target variable to confirm what we've seen above.","metadata":{}},{"cell_type":"code","source":"target = 'accident_risk'\ndata_train[target].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_train[target].hist(bins = 20)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Next up let's look at the numerical features:","metadata":{}},{"cell_type":"code","source":"bool_features = ['road_signs_present', 'public_road', 'holiday', 'school_season']\nnum_features = ['num_lanes', 'curvature', 'speed_limit', 'num_reported_accidents']\n\ndata_train[num_features].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"data_train[bool_features].describe()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"Now let's look at the categorical features:","metadata":{}},{"cell_type":"code","source":"cat_features = ['road_type', 'lighting', 'weather', 'time_of_day']\nfor col in cat_features:\n    sns.barplot(data = data_train, x = col, y = target)\n    plt.show()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Data Processing and Feature Engineering\n\n## Data Processing\nThe first step is to clean and format the raw data.\n* Missing Values: A check with .info() confirms the dataset is complete, with no null values. This is great news, as it means we don't need to perform imputation or drop any data.\n* Boolean features: The boolean features are converted from True/False to their integer equivalents 1/0.\n* Categorical features: The categorical features are OHEd through a pipeline and column tranformer\n\n## Feature Engineering\nTo help the model find more complex patterns, we create several new features from the existing data.\n* accidents_per_lane: We create a ratio to get a measure of accident density. This is more predictive than just the raw accident count.\n* weather_and_time: We create an interaction feature to capture the combined risk of these two conditions.\n* square_speed: We create a polynomial feature to help the model capture the non-linear relationship between speed and risk, as risk often increases exponentially with speed.","metadata":{}},{"cell_type":"code","source":"def new_features(df):\n    df_new = df.copy()\n\n    df_new['accidents_per_lane'] = df_new['num_reported_accidents'] / (df_new['num_lanes'])\n    df_new['weather_and_time'] = df_new['weather'] + df_new['time_of_day']\n    df_new['square_speed'] = df_new['speed_limit'] ** 2\n\n    return df_new\n\ny_train = data_train[target]\nx_train = data_train.drop(columns = ['id', target])\n\nids = data_test['id']\nx_test = data_test.drop(columns = ['id'])\n\nfor col in bool_features:\n    x_train[col] = x_train[col].astype(int)\n    x_test[col] = x_test[col].astype(int)\n\nx_train = new_features(x_train)\nx_test = new_features(x_test)\n\nnum_features.extend(['accidents_per_lane', 'square_speed'])\ncat_features.append('weather_and_time')\nfeatures = num_features + bool_features\n\ncat_transformer = Pipeline(steps = [\n    ('OH', OneHotEncoder(handle_unknown = 'ignore', drop = 'first', sparse_output = False))\n])\n\npreproc = ColumnTransformer(\n    transformers = [\n        ('OH', cat_transformer, cat_features),\n        ('pass', 'passthrough', features)\n    ], \n    remainder = 'drop')","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Hyperparameter Tuning with Optuna\n\nInstead of guessing parameters manually, Optuna automates the search. We define an objective function where, for each trial, Optuna:\n1.  Suggests a new combination of hyperparameters (learning_rate, max_depth, etc).\n2.  Runs our entire 10-fold cross-validation loop using those parameters.\n3.  Receives the final, reliable oof rmses score from that CV.\n\nOptuna repeats this process 25 times, learning from each trial to make smarter guesses.","metadata":{}},{"cell_type":"code","source":"def objective(trial):\n    \n    params = {\n        'objective': 'reg:squarederror',\n        'n_estimators': 2000,\n        'early_stopping_rounds': 50,\n        'n_jobs': -1,\n        'random_state': 3,\n        \n        'tree_method': 'hist',\n        'device': 'cuda',\n\n        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.1, log=True),\n        'max_depth': trial.suggest_int('max_depth', 3, 10),\n        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n        'reg_alpha': trial.suggest_float('reg_alpha', 1e-8, 1.0, log=True),\n        'reg_lambda': trial.suggest_float('reg_lambda', 1e-8, 1.0, log=True),\n    }\n    \n    folds = 10\n    kf = KFold(n_splits = folds, shuffle = True, random_state = 1)\n\n    oof_preds = np.zeros(x_train.shape[0])\n    fold_scores = []\n\n    for fold, (train_id, val_id) in enumerate(kf.split(x_train, y_train)):\n\n        x_train_fold = x_train.iloc[train_id]\n        y_train_fold = y_train.iloc[train_id]\n        x_val_fold = x_train.iloc[val_id]\n        y_val_fold = y_train.iloc[val_id]\n\n        x_train_proc = preproc.fit_transform(x_train_fold)\n        x_val_proc = preproc.transform(x_val_fold)\n        \n        model = xgb.XGBRegressor(**params)\n\n        model.fit(x_train_proc, y_train_fold,\n            eval_set = [(x_val_proc, y_val_fold)],\n            verbose = False\n        )\n        \n        val_preds = model.predict(x_val_proc)\n        oof_preds[val_id] = val_preds\n        \n        rmse = np.sqrt(mean_squared_error(y_val_fold, val_preds))\n        fold_scores.append(rmse)\n    \n    final_oof_rmse = np.sqrt(mean_squared_error(y_train, oof_preds))\n    \n    trial.report(final_oof_rmse, step=0)\n    if trial.should_prune():\n        raise optuna.exceptions.TrialPruned()\n\n    return final_oof_rmse\n\n\nstudy = optuna.create_study(direction = 'minimize')\nstudy.optimize(objective, n_trials = 25)\n\nbest_params = study.best_params\nprint(best_params)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Final Model Training and Evaluation\n\nWe run our 10-fold cross-validation loop one last time. This isn't for tuning; this is to generate our final predictions.\n\n1.  Generates final predictions: Each of the 10 models, trained on its fold, makes predictions on the test set. We average all 10 of these predictions to create a single, robust test_preds_final. \n\n2.  Confirms final score: We get a final (mean) rmse from this run. This score is our best and most reliable estimate of how our model will perform on new, unseen data.","metadata":{}},{"cell_type":"code","source":"best_params['objective'] = 'reg:squarederror'\nbest_params['n_estimators'] = 3000\nbest_params['early_stopping_rounds'] = 50\nbest_params['n_jobs'] = -1\nbest_params['random_state'] = 4\nbest_params['tree_method'] = 'hist'\nbest_params['device'] = 'cuda'\n\nfolds = 10\nkf = KFold(n_splits = folds, shuffle = True, random_state = 5)\n\noof_preds_final = np.zeros(x_train.shape[0])\ntest_preds_final = np.zeros(x_test.shape[0])\nfinal_fold_scores = []\n\nfor fold, (train_id, val_id) in enumerate(kf.split(x_train, y_train)):\n    \n    x_train_fold = x_train.iloc[train_id]\n    y_train_fold = y_train.iloc[train_id]\n    x_val_fold = x_train.iloc[val_id]\n    y_val_fold = y_train.iloc[val_id]\n\n    x_train_proc = preproc.fit_transform(x_train_fold)\n    x_val_proc = preproc.transform(x_val_fold)\n    x_test_proc_fold = preproc.transform(x_test)\n\n    model = xgb.XGBRegressor(**best_params)\n    \n    model.fit(x_train_proc, y_train_fold,\n        eval_set = [(x_val_proc, y_val_fold)],\n        verbose = 500\n    )\n    \n    val_preds = model.predict(x_val_proc)\n    oof_preds_final[val_id] = val_preds\n    \n    rmse = np.sqrt(mean_squared_error(y_val_fold, val_preds))\n    final_fold_scores.append(rmse)\n\n    test_preds_final += model.predict(x_test_proc_fold) / folds\n\n\nrmse = np.mean(final_fold_scores)\nprint(rmse)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"markdown","source":"# Conclusion\n\nAnd there we go! This project successfully built a complete, high-performance pipeline for predicting road accident risk.\n\nWe began with a simple model, established a reliable baseline RMSE of 0.05619 using KFold cross validation, and then performed automated hyperparameter tuning with Optuna to find the best possible settings for our model.\n\nOur final, tuned model achieved an rmse of 0.05603.\n\nThis is a very strong and complete result. While there are always more steps to take, we have successfully moved from a simple baseline to a fully optimized and robust machine learning solution.","metadata":{}},{"cell_type":"code","source":"sub = pd.DataFrame({'id': ids, 'accident_risk': test_preds_final})\nsub.to_csv('submission.csv', index = False)","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}